#download and transfer .faa files from IMG into appropriate project folders using cyberduck
#Create directory to hold/organize all of the .faa files. 
#within the working directory there needs to be the target .faa sequence file and the program file (code is that found and ran for the hmmsearch command)

#to open the correct environment to run the programs
  conda activate hgcAB
  
# need to use cd (change directory) to open to the correct folders containing necessary data to run program pipeline

# code for running the .faa files with the hmm hgcAB search command pipeline:
# example from running on bin 01 depth 10
# note: right clicking pastes copied code
   hmmsearch -E 1e-50 --tblout bin01_10-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin01_10.faa
   
   # note that the hgcA.hmm model needs to be accessible for the code to run, this pathway specifies that the hgcA.hmm model is in the directory HgcAB
   
#to convert .out files into more usable and readable csv and comma delineated files. The output name will need to be changed for each run:

  bash ~/data/HgcAB/hmmscan_rough_parse.sh test.parse.out.csv
  
 #Note: the way that this command is written, it must be within the same folder as the .out file(s)
 #Note: if desired, if multiple .out files are within the same folder and the bash script is ran, it will compile all of the .out files and put them into a singular .csv

#If ran together, could create the .out and the .csv in the same steps
 
Bin01
 hmmsearch -E 1e-50 --tblout bin01_30-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin01_30.faa
  bash ~/data/HgcAB/hmmscan_rough_parse.sh bin01_30hgcAhits.csv
  
    hmmsearch -E 1e-50 --tblout bin01_60-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin01_60.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin01_60hgcAhits.csv
    
Bin02
     hmmsearch -E 1e-50 --tblout bin02_10-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin02_10.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin02_10hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin02_30-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin02_30.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin02_30hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin02_60-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin02_60.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin02_60hgcAhits.csv
    
Bin05
     hmmsearch -E 1e-50 --tblout bin05_10-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin05_10.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin05_10hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin05_30-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin05_30.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin05_30hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin05_60-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin05_60.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin05_60hgcAhits.csv
    
    
 Bin06
     hmmsearch -E 1e-50 --tblout bin06_10-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin06_10.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin06_10hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin06_30-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin06_30.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin06_30hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin06_60-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin06_60.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin06_60hgcAhits.csv   
    
Bin10
     hmmsearch -E 1e-50 --tblout bin10_10-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin10_10.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin10_10hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin10_30-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin10_30.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin10_30hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin10_60-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin10_60.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin10_60hgcAhits.csv    
    
Bin11
    hmmsearch -E 1e-50 --tblout bin11_10-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin11_10.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin11_10hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin11_30-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin11_30.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin11_30hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin11_60-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin11_60.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin11_60hgcAhits.csv        
    
Bin17
    hmmsearch -E 1e-50 --tblout bin17_10-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin17_10.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin17_10hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin17_30-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin17_30.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin17_30hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin17_60-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin17_60.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin17_60hgcAhits.csv      
    
Bin23
    hmmsearch -E 1e-50 --tblout bin23_10-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin23_10.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin23_10hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin23_30-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin23_30.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin23_30hgcAhits.csv
    
    hmmsearch -E 1e-50 --tblout bin23_60-hgcA-hits.out /data/home/mrpeter1/HgcAB/hgcA.hmm bin23_60.faa
    bash ~/data/HgcAB/hmmscan_rough_parse.sh bin23_60hgcAhits.csv        
    
#PULLING HITS FROM METAGENOMES AFTER HMM   
#Concatenates metaG .faa files into a single file
  cat *assembled.faa > MetaG_T.faa
#Selects and cuts the hgcA hit names that came from the metaG .faa files using the hmm from McDaniel(2020)
  cut -d , -f 1 hgcA_hits_IMG_faa_data.csv > hit_names.txt
#Converts multi line .faa files for both metaG and metaT into single lines for each sequence entry
 awk '!/^>/ { printf "%s", $0; n = "\n" } /^>/ { print n $0; n = "" } END { printf "%s", n }' MetaG_T.faa > MetaG_T_one_line.faa
#For each hit name resulting from the hmm, match that hit name and pull the sequence information from the corresponding/matching line in the concatenated .faa file
 for hit in $(hit_names.txt); do grep -A 1 $hit MetaG_T_one_line.faa ; done>hits.fasta
 
 _________________________________________________________________

#RUNNING HMM ON SINGLE CELLS
#All SC files were concatenated into a single file (all *proteins.faa)
  cat *proteins.faa > SCG.faa
#The files were converted into single-line sequence code
  awk '!/^>/ { printf "%s", $0; n = "\n" } /^>/ { print n $0; n = "" } END { printf "%s", n }' SCG.faa > SCG_one_line.faa
#hgcA hmm ran on all SC fasta
#note: hmm had to be within the working directory, dup_hgcA.hmm is the same hmm, just a copy
  hmmsearch -E 1e-50 --tblout scg-hgcA-hits.out dup_hgcA.hmm SCG_one_line.faa
  
  _________________________________________
  
#RUNNING HMM ON metaT_G COMBINATION
#All metatranscriptomic and metagenomic files were concatenated into a single file 
  cat *assembled.faa > meta_TG.faa
  
#The files were converted into single-line sequence code
  awk '!/^>/ { printf "%s", $0; n = "\n" } /^>/ { print n $0; n = "" } END { printf "%s", n }' meta_TG.faa > meta_TG_one_line.faa
  
#hgcA hmm ran on all metaG and metaT fasta
#note: hmm had to be within the working directory, dup_hgcA.hmm is the same hmm, just a copy
  hmmsearch -E 1e-50 --tblout meta_TG-hgcA-hits.out hgcA_dup.hmm meta_TG_one_line.faa

#converting the hits from .out to .csv
#note: the hmmscan command is in the wider HgcAB
  bash ~/data/HgcAB/hmmscan_rough_parse2.sh meta_TG-hgcA-hits.out meta_TG-hgcA-hits-out.csv
  
#Pulling hit names from the csv and creating a txt list
  cut -d , -f 1 meta_TG-hgcA-hits-out.csv > MetaG_T-names.txt
  
INCOMPLETE/NON-WORKING CODE
#PUlling all other sequences from the hgcA-containing contigs
  for hit in $(cat MetaG_T-names.txt); do grep $hit ~/data/HgcA/Maddy_gff/MetaG_T.gff|cut -d$'\t' -f 1 ; done > meta_TG-contigs.txt
  for hit in $(cat meta_TG-contigs.txt); do grep $hit Maddy_gff/MetaG_T.gff|cut -d$'\t' -f 9| cut -d ';' -f 2 | cut -d '=' -f 2 ; done > meta_TG-contigs_hits.txt
  for hit in $(cat meta_TG-contigs_hits.txt); do grep -A 1 $hit meta_TG_one_line.faa ; done> meta_TG-contig_hits.fasta
 
##PULLING hgcA CONTIG IDs FROM GFF FILES AND SEARCHING FOR hgcB

#PULL only contig names/info from the .gff files using the hgcA hit list from the hmm
  for hit in $(cat MetaG_T-names.txt); do grep $hit ~/data/HgcAB/Maddy_gff/MetaG_T.gff|cut -d$'\t' -f 1 ; done > Contig_ID_of_HgcA_contigs.txt
#COUNT HOW MANY GENES ARE ON EACH PULLED CONTIG
  for hit in $(cat Contig_ID_of_HgcA_contigs.txt); do echo $hit; grep -c $hit ~/data/HgcAB/Maddy_gff/MetaG_T.gff; done > Count_of_genes_on_HgcA_contigs.txt
#PULL the gene ID names as .txt file from contig list in the first part (All genes on all the contigs from that list)
  for hit in $(cat Contig_ID_of_HgcA_contigs.txt); do grep $hit ~/data/HgcAB/Maddy_gff/MetaG_T.gff|cut -d$'\t' -f 9| sed -n 's/.*locus_tag=//p'| cut -d ';' -f 1; done > MetaG_T_gene_ID_HgcA-contigs_hits.txt
#USING the list of contig names, pull all the gene sequences from the identified contigs
  for hit in $(cat MetaG_T_gene_ID_HgcA-contigs_hits.txt); do grep -A 1 $hit MetaG_T_one_line.faa ; done>MetaG_T-contig_hits.fasta
#CLEANS UP THE FILES BY REMOVING DASHES
    sed ':a;N;$!ba;s/\n--\n/\n/g' MetaG_T-contig_hits.fasta > MetaG_T-contig_hits2.fasta
  
#SEARCH THE hgcA CONTIIGS FOR hgcB 
#note: hmm scripts are labeled as 'profiles' in the McDaniel figshare page. The hgcB.hmm script hgcB-profile
  hmmsearch -E 1e-50 --tblout MetaG_T-hgcB-hits.out hgcB.HMM MetaG_T-contig_hits2.fasta
  
IMCOMPLETE/NON-WORKING CODE
#CONVERT .out HITS TO CSV
  bash ~/data/HgcAB/hmmscan_rough_parse2.sh MetaG_T-hgcB-hits.out MetaG_T-hgcB-hits-out.csv

#SOLUTION
#CONVERT .out HITS TO CSV
#if e value threshold is reduced, we obtain more hits
  bash ~/data/HgcAB/hmmscan_rough_parse2.sh Steve_test_HB_all.out Steve_test_HB_all.out.csv
  
##BUILDING PHYLOGENETIC TREE

#DOWNLOAD NECESSARY PACKAGES TO CREATE A PHYLOGENY ENVIRONMENT
  conda create -n phylogeny -c bioconda -c conda-forge clustalo quicktree \
             muscle phyml modeltest-ng fasttree raxml
             
#ACTIVATE ENVIRONMENT
  conda activate phylogeny

#EXTRACT hgcA SEQUENCES USING hgcA hits taken from hmm of meta_GT (197/200 total hgcA hits)
  for hit in $(cat MetaG_T-names.txt); do grep -A 1 $hit meta_TG_one_line.faa ; done > hgcA_hit_sequences.fasta

#PERFORM a muscle alignment with Clustal and output as a stockholm format
  clustalo -i hgcA_hit_sequences.fasta -o hgcA_hit_sequences.st --outfmt st
  
#CONSTRUCT a neighbor joining tree with 1000 bootstraps
NOTE: screen -RD muscle
  quicktree -boot 1000 hgcA_hit_sequences.st > clustal_hgcA_tree.nwk
  

#CREATE A MULTIPLE SEQUENCE ALIGNMENT OF hgcA USING MUSCLE
  muscle -in hgcA_hit_sequences.fasta -out meta_hgcA_Muscle.fa -phyiout meta_hgcA_phyi.out -physout meta_hgcA_phys.out

#TEST FOR BEST MODEL TO BUILD TREE
NOTE: screen -RD muscle2
  modeltest-ng -i meta_hgcA_phys.out -o HSP20_models -t ml -d aa -p 5
  
NOTE: ERROR saying there are duplicate sequence headers for the hgcA seqeunces
  
#RUN THE BEST MODEL TO BUILD MAXIMUM LIKELIHOOD TREE

________________________________________________________________________

#RENAMING hgcA metaG Compiled + metaT AND RUNNING MUSCLE 

#PULLING AND CONVERTING NEW NAMES FROM .csv TO .txt LIST
  cut -d , -f 1 hgcA_all_compiled.csv > hgcA_GT_compiled.txt

NOTE: did not get to this step with the previous files, as it was decided that having the hgcAB combination of protein sequences was more time efficient.
#CREATE A MULTIPLE SEQUENCE ALIGNMENT OF compiled hgcA USING MUSCLE
  muscle -in hgcA_hit_sequences.fasta -out meta_hgcA_Muscle.fa -phyiout meta_hgcA_phyi.out -physout meta_hgcA_phys.out
  
____________________________________

#EXTRACTING A LIST OF CONTIG NAMES FOR THE hgcB proteins

#CONVERT LIST TO .TXT
  cut -d , -f 1 Steve_test_HB_all.out.csv > HgcB-names.txt
#FOR EVERY NAME IN TXT FIND AND PULL CONTIG INFO
  for hit in $(cat HgcB-names.txt); do grep $hit ~/data/HgcAB/Maddy_gff/MetaG_T.gff|cut -d$'\t' -f 1 ; done > Contig_ID_of_HgcB_contigs.txt
NOTE: this did not pull contig info it seems, but just re-pulled the same gene names as before, but with the addition of eliminating the last character from the end of the sequence name.

#CORRECTED CODE TO PULL CONTIGS
  for hit in $(cat HgcB-names.txt); do grep -w $hit ~/data/HgcAB/Maddy_gff/MetaG_T.gff|cut -d$'\t' -f 1 ; done > Contig_ID_of_HgcB_contigs.txt

____________________________________________________

#USING A/B TXT LIST FROM CONTIG MATCHING, PULL SEQUENCES
   for hit in $(cat hgcA_hits_contig-matching.txt); do grep -A 1 $hit meta_TG_one_line.faa ; done > hgcA_contig-matching.fasta
   
   for hit in $(cat hgcB_hits_contig-matching.txt); do grep -A 1 $hit meta_TG_one_line.faa ; done > hgcB_contig-matching.fasta
   
#CONVERTING THE .CSV FILE INTO A .TXT FILE
   cut -d , -f 1 hgcAB_combined_sequences.csv > hgcAB_combined_sequences.txt
   cut -d , -f 1 hgcAB_combined_sequences_nospace.csv > hgcAB_combined_sequences_nospace.txt
NOTE: this file still contains spaces in the sequences 

#CORRECTED SEQUENCE FILE WITH NO SPACING
  cut -d , -f 1 hgcAB_combined_sequences_nospace2.csv > hgcAB_combined_sequences_nospace2.txt
  cut -d , -f 1 hgcAB_combined_sequences_nospace2.csv > hgcAB_combined_sequences_nospace2.fasta

__________________________________________________________

#PERFORM a muscle alignment with Clustal and output as a stockholm format
  clustalo -i hgcAB_combined_sequences_nospace2.fasta -o hgcAB_combined.st --outfmt st
NOTE: program warning- formatting issues, cannot read file

If you run the command less, which has a weird <U+FEEF> symbol at the beginning
  less hgcAB_combined_sequences_nospace2.txt
You can run nano to edit the file on the server  
  nano hgcAB_combined_sequences_nospace2.txt
If you delete this space and run: 
  less hgcAB_combined_sequences_nospace2.txt     that symbol is no longer there
NOTE: still will not run in clustal

#REMOVING * WITHIN THE SEQUENCES
  cut -d , -f 1 hgcAB_combined_nospc_noast.csv > hgcAB_combined_nospc_noast.fasta
  nano hgcAB_combined_nospc_noast.fasta
  
Multiple sequence alignment
  clustalo -i hgcAB_combined_nospc_noast.fasta -o hgcAB_combined.st --outfmt st
Building neighbor joining tree
  quicktree -boot 1000 hgcAB_combined.st > clustal_hgcAB_combined.nwk
  
  __________________________________________________________________
  
Converting McDaniel .aln file to .txt to combine with PEATcosm data
  cut -d , -f 1 hgcAB-select-sequences-cleaned-fused-root.aln > McD_hgcAB-select-sequences-cleaned-fused-root.txt
Converting sequences to one line
  awk '!/^>/ { printf "%s", $0; n = "\n" } /^>/ { print n $0; n = "" } END { printf "%s", n }' McD_hgcAB-select-sequences-cleaned-fused-root.txt > McD_hgcAB-select-sequences-cleaned-fused-root_oneline.txt
  
Convert newly combined McDaniel and PEATcosm hits into .fasta file
  cut -d , -f 1 McD_combined_hgcAB.txt > McD_combined_hgcAB.fasta
  
Create clustal tree for combined data
  Multiple sequence alignment
    clustalo -i McD_combined_hgcAB.fasta -o McD_combined_hgcAB.st --outfmt st
  NOTE: this brings up error stating profile has no leading or trailing residues and will not produce an output in the next step
    quicktree -boot 1000 McD_combined_hgcAB.st > McD_combined_hgcAB.nwk
 TROUBLESHOOTING:
 Coverting the McDaniel data to CSV in order to remove dashes from McDaniel data
  cut -d , -f 1 McD_combined_hgcAB.fasta > McD_combined_hgcAB.csv
  cut -d , -f 1 McD_combined_hgcAB_nodash.csv > McD_combined_hgcAB_nodash.faa

Multiple sequence alignment
    clustalo -i McD_combined_hgcAB_nodash.faa -o McD_combined_hgcAB_nodash.st --outfmt st
    quicktree -boot 1000 McD_combined_hgcAB_nodash.st > McD_combined_hgcAB_nodash.nwk
    #successful tree

    DOESNT WORK
    muscle -in McD_combined_hgcAB_nodash.faa -out McD_combined_hgcAB_nodash_MUSCLE.fa -phyiout McD_combined_hgcAB_nodash.out -physout McD_combined_hgcAB_nodash.out
    modeltest-ng -i McD_combined_hgcAB_nodash.out -o McD_combined hgcAB_models -t ml -d aa -p 5
    NOTE: can't run- same problem before with the "dupliate names", titles for sequences can only be max 10 characters   
    
SOLUTION
#AFTER RENAMING McDANIEL DATA AND PEATcosm DATA (UNDER 10 CHARACTER)

Convert to faa file
  cut -d , -f 1 McD_combined_hgcAB_UniqueRenamed.csv > McD_combined_hgcAB_UniqueRenamed.faa
  
  #REMOVED HIDDEN CODE AT BEGINNING (thus why input is ..Renamed2.faa)
  muscle -in McD_combined_hgcAB_UniqueRenamed2.faa -out McD_combined_hgcAB_UniqueRenamed_MUSCLE.fa -phyiout McD_combined_hgcAB_UniqueRenamed.out -physout McD_combined_hgcAB_UniqueRenamed.out
 
 modeltest-ng -i McD_combined_hgcAB_UniqueRenamed.out -o McD_combined_hgcAB_models -t ml -d aa -p 5
  #NOTE: modeltest aborted
  
  Removed last specifications in code in effort to redue amount of memory being utilized
  modeltest-ng -i McD_combined_hgcAB_UniqueRenamed.out -o McD_combined_hgcAB_models -t ml -d aa
  #NOTE: still aborted
  
ModelTesting with fastree (only three models)
  modeltest-ng -i McD_combined_hgcAB_UniqueRenamed.out -o McD_combined_hgcAB_Uniquemodels -t ml -d aa -m WAG,JTT,LG
RESULTS:
Partition 1/1:
                         Model         Score        Weight
----------------------------------------------------------
       BIC           LG+I+G4+F   312688.6205        1.0000
       AIC           LG+I+G4+F   306117.2512        1.0000
      AICc               LG+G4  4252077.2573        1.0000

Creating an aligment file (aln) for the combined hgcAB sequences
  muscle -in McD_combined_hgcAB_UniqueRenamed2.faa -out McD_combined_hgcAB_UniqueRenamed_MUSCLE.aln 

Running the model to create a tree
  fasttree -lg -gamma McD_combined_hgcAB_UniqueRenamed_MUSCLE.aln > gamma_McD_combined_hgcAB_fasttree.nwk
  
____________________________________________________________________________________________________

PULLING HGCA-CONTAINING SCAFFOLDS

#COPY ALL .FASTA FILES FROM SUB FOLDERS TO ONE NEW FOLDER
  find . -name *scaffolds.fasta -exec cp '{}' "/home/mrpeter1/data/HgcAB/scaffolds/cat_scaffolds/" ";"
  NOTE: https://unix.stackexchange.com/questions/67503/move-all-files-with-a-certain-extension-from-multiple-subdirectories-into-one-di
  
#CONCATENATE ALL *SCAFFOLD.FASTA FILES
  cat /home/mrpeter1/data/HgcAB/scaffolds/cat_scaffolds *scaffolds.faa > all_scaffolds.fasta
  NOTE: scaffold naming convention contains duplicates from numerical organization

*Testing*
#PULLING SCAFFOLD SEQS FROM BINS INDIVIDUALLY
Bin_01_10
  for hit in $(cat bin1_10_contigs.csv); do grep -A 1 $hit /home/mrpeter1/data/HgcAB/scaffolds/PEATcosm2014_Bin_63/ >bin1_10_contig_seqs.txt
  NOTE: didn't work
  
*TESTING*: Trying to pull sequences with partial matches of string label
   for hit in $(cat bin1_10_contigs.csv); do grep $hit /home/mrpeter1/data/HgcAB/scaffolds/PEATcosm2014_Bin_63/QC_and_genome_Assembly/PEATcosm2014_Bin_63/ >bin1_10_contig_seqs.txt
   NOTE: didn't work
   
This section was instead completed manually.
_________________________________________________________________________________________________________

##BUILDING A PHYLOGENETIC TREE OF hgcA-HIT SCAFFOLDS

#CHANGING SCAFFOLS SEQUENCE FILE FROM .txt TO .fasta
  cut -d , -f 1 scaffold_hgcA_nucleicacid_sequences.txt > scaffold_hgcA_nucleicacid_sequences.fasta
 
  #NON-WORKING COMMMANDS
  #CONVERTING SCAFFOLD SEQUENCES INTO SINGLE-LINE
  awk '!/^>/ { printf "%s", $0; n = "\n" } /^>/ { print n $0; n = "" } END { printf "%s", n }' scaffold_hgcA_nucleicacid_sequences.fasta > scaffold_hgcA_nucleicacid_sequences_one_line.fasta
  awk '{if(NR==1) {print $0} else {if($0 ~ /^>/) {print "\n"$0} else {printf $0}}}' scaffold_hgcA_nucleicacid_sequences.fasta > scaffold_hgcA_nucleicacid_sequences_one_line2.fasta

SOLUTION
  awk '!/^>/ { printf "%s", $0; n = "\n" } /^>/ { print n $0; n = "" } END { printf "%s", n }' scaffold_hgcA_nucleicacid_sequences.fasta > scaffold_hgcA_nucleicacid_sequences_one_line.fasta
#Note:The output of the above command results in a file containing hidden ^M symbols, that can be removed by the following command:
#REMOVING HIDDEN '^M' IN SEQUENCES
  sed -e "s/^M//g" scaffold_hgcA_nucleicacid_sequences_one_line.fasta >scaffold_hgcA_nucleicacid_sequences_one_line2.fasta 

#CONVERTING SINGLE-LINE FASTA INTO .txt
  cut -d , -f 1 scaffold_hgcA_nucleicacid_sequences_one_line2.fasta > scaffold_hgcA_nucleicacid_sequences_one_line2.txt
  Note: This results in the sequences not falling into single-line form, but can be seen as single line with nano in fasta form
  cut -d , -f 1 scaffold_hgcA_nucleicacid_sequences_one_line2.fasta > scaffold_hgcA_nucleicacid_sequences_one_line2.csv

 
NOTE: Sequence names were given new names under 10 characters. PEATxxx names given to hgcAB sequences will match new PEAT scaffold names, and all other will be given numbers following the last assignment.
NOTE: PEATxxx names from hgcAB sequences from PEAT001 to PEAT49

#CONVERTING FROM CSV TO TXT
  cut -d , -f 1 scaffold_hgcA_seq_unique_name_list.csv > scaffold_hgcA_seq_unique_name_list.txt

#NON-WORKING
 #REPLACING FASTA HEADERS WITH UNIQUE HEADERS (PEATxxx)
 awk 'FNR==NR {
    hash[">"$1]=$2;
    next;}
if ($1 ~ /^>/) { print ">"hash[$1];} else { print $1;}' scaffold_hgcA_seq_unique_name_list.txt scaffold_newname.fasta
#NON-WORKING
#CONVERTING FROM CSV TO TXT
  cut -d , -f 1 scaffold_hgcA_seq_unique_name_list.csv > scaffold_hgcA_seq_unique_name_list.txt
  awk 'FNR==NR { a[">"$1]=$2; next } $1 in a { sub(/>/,">"a[$1]"|",$1)}1' scaffold_hgcA_seq_unique_name_list2.txt scaffold_hgcA_nucleicacid_sequences_one_line2.fasta
#NON-WORKING
#CONVERTING FROM CSV TO TXT (adding back > at beginning of seq name)
  cut -d , -f 1 scaffold_hgcA_seq_unique_name_list2.csv > scaffold_hgcA_seq_unique_name_list2.txt
  awk 'FNR==NR { a[">"$1]=$2; next } $1 in a { sub(/>/,">"a[$1]"|",$1)}1' scaffold_hgcA_seq_unique_name_list2.txt scaffold_hgcA_nucleicacid_sequences_one_line2.fasta
  
#NON-WORKING  
awk 'FNR==NR {
    hash[">"$1]=$2;
    next;
}

if ($1 ~ /^>/) {
    print ">"hash[$1];
} else {
    print $1;
}' scaffold_hgcA_seq_unique_name_list2.txt scaffold_hgcA_nucleicacid_sequences_one_line2.fasta

#NON-WORKING
#CONVERTING FROM CSV TO TXT (Putting in format of >New|Old)
  cut -d , -f 1 scaffold_hgcA_seq_unique_name_list3.csv > scaffold_hgcA_seq_unique_name_list3.txt
  awk 'FNR==NR { a[">"$1]=$2; next } $1 in a { sub(/>/,">"a[$1]"|",$1)}1' scaffold_hgcA_seq_unique_name_list3.txt scaffold_hgcA_nucleicacid_sequences_one_line2.fasta
  
SOLUTION
#CONVERTING FROM CSV TO TXT (old=new format) https://community.unix.com/t/how-to-replace-fasta-headers-based-on-the-given-list-txt/380260

  cut -d , -f 1 scaffold_hgcA_seq_unique_name_list4.csv > scaffold_hgcA_seq_unique_name_list4.txt
  awk -F= 'FNR==NR {f2[$1]=$2;next} $2 in f2 {$2=f2[$2]}1' scaffold_hgcA_seq_unique_name_list4.txt FS='>' OFS='>' scaffold_hgcA_nucleicacid_sequences_one_line2.fasta > scaffold_unique_name.fasta

  _____________________________________________________________________

#RUNNING (SIMPLE) ALIGNMENT AND BUILDING SIMPLE TREE
    clustalo -i scaffold_unique_name.fasta -o scaffold_unique_name.st --outfmt st
 NOTE: alignment aborted
    quicktree -boot 1000 scaffold_unique_name.st > scaffold_unique_name.nwk

#RUNNING MORE HEFTY ALIGNMENT AND CREATING ALIGNMENT FILE (.aln)
  muscle -in scaffold_unique_name.fasta -out scaffold_unique_name.aln -phyiout scaffold_unique_name.out -physout scaffold_unique_name.out
NOTE: alignment aborted

________________________________________________________________________
Correcting for initial incomplete lists
_________________________________________________________________________

#NOTE: .txt files need to be in the proper format, 
File type can be checked using the command nano filename.txt 
  1) When ever making an input text file, you could make the text file on you PC, then on the command line type in nano nameoffile.  This will open up the nano text editor.  You can then paste the contents of your text file and then save the file. This will format the text file properly.
  2) Convert using Notepad++ or dos2unix foo.txt command

#create txt file using nano and pasting in data 
  nano corrected_hgcA_contigs200.txt
  
#pull all gene ID names from hgcA-containing contigs
  for hit in $(cat corrected_hgcA_contigs200.txt); do grep $hit ~/data/HgcAB/Maddy_gff/MetaG_T.gff|cut -d$'\t' -f 9| sed -n 's/.*locus_tag=//p'| cut -d ';' -f 1; done > corrected200_gene_ID_HgcA-contigs_hits.txt
#USING the list of all gene names, pull all the gene sequences from the identified contigs
  for hit in $(cat corrected200_gene_ID_HgcA-contigs_hits.txt); do grep -A 1 $hit MetaG_T_one_line.faa ; done>corrected200_hgcAcontigs_all_genes.fasta
  NOTE: the above .fasta contains -- between ~15 entries

 #removing -- characters from file
 sed -e "s/--//g" corrected200_hgcAcontigs_all_genes.fasta >corrected200_hgcAcontigs_all_genes_nodash.fasta
 #NOTE: used grep . file.txt command to remove empty lines

#search above list of genes with hgcB hmm
  hmmsearch -E 1e-25 --tblout corrected200_hgcB_genes.out hgcB.HMM corrected200_hgcAcontigs_all_genes_nodash.fasta
  NOTE: original 1e-50 was too strict a criteria and came back with no hits, lowering threshold to 25 resulted in 92 hit matches above the threshold
  NOTE: this resulted in duplicate hits, where hit is listed more than once for a number of sequences, following line tests wehtehr empty line removal influences hmm
#re-ran with file with removed empty lines
  hmmsearch -E 1e-25 --tblout corrected200_hgcB_genes2.out hgcB.HMM corrected200_hgcAcontigs_all_genes_nodash.fasta
  NOTE: no difference in output
  

  
#CONVERT .out HITS TO CSV
#if e value threshold is reduced in hmmsearch step, we obtain more hits
  bash ~/data/HgcAB/hmmscan_rough_parse2.sh corrected200_hgcB_genes.out corrected200_hgcB_genes.csv
  
screen -r Bhmm
__________________________________________________________________

#EXTRACTING LIST OF CONTIG NAMES FOR THE hgcB proteins

#using nano, paste new B hit names into txt file
  corrected200_hgcB_names.txt
#FOR EVERY NAME IN TXT FIND AND PULL CONTIG INFO
  for hit in $(cat corrected200_hgcB_names.txt); do grep $hit ~/data/HgcAB/Maddy_gff/MetaG_T.gff|cut -d$'\t' -f 1 ; done > corrected_contig_ID_of_HgcB_contigs.txt
# DIFFERENT CODE TO PULL CONTIGS
  for hit in $(cat corrected200_hgcB_names.txt); do grep -w $hit ~/data/HgcAB/Maddy_gff/MetaG_T.gff|cut -d$'\t' -f 1 ; done > corrected_contig_ID_of_HgcB_contigs2.txt
____________________________________________________________________

B extraction test


for hit in $(cat corrected_hgcA_contigs200.txt); do echo $hit; grep -w -c $hit ~/data/HgcAB/Maddy_gff/MetaG_T.gff; done > test_Count_of_genes_on_HgcA_contigs.txt
for hit in $(cat corrected_hgcA_contigs200.txt); do grep -w $hit ~/data/HgcAB/Maddy_gff/MetaG_T.gff|cut -d$'\t' -f 9| sed -n 's/.*locus_tag=//p'| cut -d ';' -f 1; done > test_MetaG_T_gene_ID_HgcA-contigs_hits.txt
for hit in $(cat test_MetaG_T_gene_ID_HgcA-contigs_hits.txt); do grep -w -A 1 $hit MetaG_T_one_line.faa ; done>test_MetaG_T-contig_hits.fasta
sed ':a;N;$!ba;s/\n--\n/\n/g' test_MetaG_T-contig_hits.fasta > test_MetaG_T-contig_hits2.fasta

hmmsearch -E 1e-30 --tblout test_MetaG_T-hgcB-hits.out hgcB.HMM test_MetaG_T-contig_hits2.fasta
bash ~/data/HgcA/hmmscan_rough_parse2.sh test_MetaG_T-hgcB-hits.out test_MetaG_T-hgcB-hits-out.csv

#pull sequences for the hits from the list
for hit in $(cat test_hgcB-names.txt); do grep -w -A 1 $hit test_MetaG_T-contig_hits2.fasta ; done>test_hgcB-hits.fasta

#pull the contig ID for the hgcB hits
  for hit in $(cat test_hgcB-names.txt); do grep -w $hit ~/data/HgcAB/Maddy_gff/MetaG_T.gff|cut -d$'\t' -f 1 ; done > test_Contig_ID_of_HgcB_contigs.txt

________________________________________________________
#Making hgcA PEAT alignment and phylogeny tree

Pull all sequences (200, both G and T)
  for hit in $(cat corrected200_hgcA_gene_sequences.txt); do grep -A 1 $hit meta_TG_one_line.faa ; done > corrected200_hgcA_gene_sequences.fasta

NOTE: Need to rename sequences to PEATxxx in order to run muscle 
#File named corrected200_PEAT_hgcA_sequences.fasta

Creating an aligment file (aln) for the combined PEAT hgcA sequences
fasta file was created and saved in MegaX as a .fasta file with new PEAT names and was able to run through muscle (corrected_PEAT_hgcA_sequences_MX.fasta)
  conda activate phylogeny
  #remove space at beginning of file
  muscle -in corrected200_PEAT_hgcA_sequences_nospace.fasta -out corrected200_PEAT_hgcA_sequences2.aln 
  
ModelTesting with fastree (only three models)
  modeltest-ng -i corrected200_PEAT_hgcA_sequences.out -o corrected200_hgcA_hits -t ml -d aa -m WAG,JTT,LG
  
Summary:

Partition 1/1:
                         Model         Score        Weight
----------------------------------------------------------
       BIC          WAG+I+G4+F    57043.8890        1.0000
       AIC          WAG+I+G4+F    55317.9460        1.0000
      AICc               LG+G4    61014.8019        0.5683



Running the model to create a tree
  fasttree -wag -gamma corrected200_PEAT_hgcA_sequences2.aln > corrected_200_hgcA_hits2.nwk
  

___________________________________________________________________________________
#CORRECTED AB COMBINED TREE (LIMITED 30 AB combined)

#File corrected McD_hgcAB_combined created in nano: corrected_McD_hgcAB_combined_MX.fasta
All * were removed

Creating an aligment file (aln) for the combined PEAT hgcA sequences
  conda activate phylogeny
  muscle -in corrected_McD_hgcAB_combined_MX.fasta -out corrected_McD_hgcAB_combined_MX.aln 
  muscle -in corrected_McD_hgcAB_combined_MX.fasta -phyiout corrected_McD_hgcAB_combined_MX.out -physout corrected_McD_hgcAB_combined_MX.out
  
ModelTesting with fastree (only three models)
  modeltest-ng -i corrected_McD_hgcAB_combined_MX.out -o corrected_McD_hgcAB_modeltest -t ml -d aa -m WAG,JTT,LG

Summary:

Partition 1/1:
                         Model         Score        Weight
----------------------------------------------------------
       BIC           LG+I+G4+F   311179.8044        1.0000
       AIC           LG+I+G4+F   304856.2569        1.0000
      AICc               LG+G4  4040227.1846        1.0000



Running the model to create a tree
  fasttree -lg -gamma corrected_McD_hgcAB_combined_MX.aln  > corrected_McD_hgcAB_combined_tree.nwk
Removed spaves in name
  fasttree -lg -gamma corrected_McD_hgcAB_combined_MX_aln.txt  > corrected_McD_hgcAB_combined_tree.nwk

____________________________________________________________________________________________________

#CORRECTED (ALL 70) AB SEQS COMBINED

#File corrected McD_hgcAB_combined created in nano: corrected_70_McD_hgcAB_combined_MX.fasta
All * were removed and spaces in sequence names removed

Creating an aligment file (aln) for the combined PEAT hgcA sequences
  conda activate phylogeny
  muscle -in corrected_70_McD_hgcAB_combined_MX.fasta -out corrected_70_McD_hgcAB_combined_MX.aln 
  muscle -in corrected_70_McD_hgcAB_combined_MX.fasta -phyiout corrected_70_McD_hgcAB_combined_MX.out -physout corrected_70_McD_hgcAB_combined_MX.out
  
  Running the model to create a tree
  fasttree -lg -gamma corrected_70_McD_hgcAB_combined_MX.aln  > corrected_70_McD_hgcAB_combined_tree.nwk
  
ModelTesting with fastree (only three models)
  modeltest-ng -i corrected_70_McD_hgcAB_combined_MX.out -o corrected_70_McD_hgcAB_modeltest -t ml -d aa -m WAG,JTT,LG 
  
________________________________________________________________________________

##PHYLOGENY HGCA PEAT (181) AND MCD (904)

#create new fasta file of sequences in nano by pasting list into new fileinto McDaniel folder: MCD_PEAT_hgcA_fasta

conda activate phylogeny
muscle -in MCD_PEAT_hgcA_fasta -out MCD_PEAT_hgcA.aln
muscle -in MCD_PEAT_hgcA_fasta -phyiout MCD_PEAT_hgcA.out -physout MCD_PEAT_hgcA.out
modeltest-ng -i MCD_PEAT_hgcA.out -o MCD_PEAT_hgcA_modeltest -t ml -d aa -m WAG,JTT,LG 

#MODELTEST OUTPUT (McDaniel folder)
Execution results written to MCD_PEAT_hgcA_modeltest.out
Starting tree written to MCD_PEAT_hgcA_modeltest.tree

Partition 1/1:
                         Model         Score        Weight
----------------------------------------------------------
       BIC           LG+I+G4+F   627026.5393        1.0000
       AIC           LG+I+G4+F   616608.2180        1.0000
      AICc               LG+G4 10024458.2707        1.0000

#CREATING TREE
fasttree -lg -gamma MCD_PEAT_hgcA.aln  > MCD_PEAT_hgcA_tree.nwk

_____________________________________________________________________
#CO-ASSEMBLY MAG HMM

#Concatenates metaG .faa files into a single file
  cat *proteins.faa > alldepth_MAG.faa
  
#Converts multi-line .faa files for MAGs into single lines for each sequence entry
 awk '!/^>/ { printf "%s", $0; n = "\n" } /^>/ { print n $0; n = "" } END { printf "%s", n }' alldepth_MAG.faa > alldepth_MAG_oneline.faa

#hgcA hmm ran on all metaG+metaT fasta
  hmmsearch -E 1e-30 --tblout alldepth_MAG_hgca_hits.out hgcA_duplicate.hmm alldepth_MAG_oneline.faa

#converting the hits from .out to .csv for readability
#note: the hmmscan command is in the wider HgcAB
  bash hmmscan_rough_parse_duplicate.sh alldepth_MAG_hgca_hits.out alldepth_MAG_hgca_hits.csv

#FOR 10CM DEPTH

#hgcA hmm ran on all metaG+metaT fasta
  hmmsearch -E 1e-30 --tblout 10depth_MAG_hgca_hits.out hgcA_duplicate.hmm Ga0455669_proteins.faa

#converting the hits from .out to .csv for readability
#note: the hmmscan command is in the wider HgcAB
  bash hmmscan_rough_parse_duplicate.sh 10depth_MAG_hgca_hits.csv 10depth_MAG_hgca_hits.out

#Converts multi-line .faa files for MAGs into single lines for each sequence entry
 awk '!/^>/ { printf "%s", $0; n = "\n" } /^>/ { print n $0; n = "" } END { printf "%s", n }' Ga0455669_proteins.faa > Ga0455669_proteins_oneline.faa

hmmsearch -E 1e-30 --tblout 10depth_MAG_hgca_hits.out hgcA_duplicate.hmm Ga0455669_proteins_oneline.faa

bash hmmscan_rough_parse_duplicate.sh 10depth_MAG_hgca_hits2.csv 10depth_MAG_hgca_hits2.out


#FOR 30CM DEPTH

#hgcA hmm ran 
  hmmsearch -E 1e-30 --tblout 30depth_MAG_hgca_hits.out hgcA_duplicate.hmm Ga0455670_proteins.faa

#converting the hits from .out to .csv for readability
#note: the hmmscan command is in the wider HgcAB
  bash hmmscan_rough_parse_duplicate.sh 30depth_MAG_hgca_hits.csv 30depth_MAG_hgca_hits.out

#Converts multi-line .faa files for MAGs into single lines for each sequence entry
 awk '!/^>/ { printf "%s", $0; n = "\n" } /^>/ { print n $0; n = "" } END { printf "%s", n }' Ga0455669_proteins.faa > Ga0455669_proteins_oneline.faa



#FOR 60CM DEPTH

#hgcA hmm ran 
  hmmsearch -E 1e-30 --tblout 60depth_MAG_hgca_hits.out hgcA_duplicate.hmm Ga0455671_proteins.faa

#converting the hits from .out to .csv for readability
#note: the hmmscan command is in the wider HgcAB
  bash hmmscan_rough_parse_duplicate.sh 60depth_MAG_hgca_hits.csv 60depth_MAG_hgca_hits.out






